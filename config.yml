data:
  tokenizer: ./assets/tokenizer.json
  train_path: ./data/goodreads_train_split.csv
  valid_path: ./data/goodreads_valid_split.csv
  test_path: ./data/goodreads_test.csv
  output_path: ./data/output.csv
  batch_size: 100
  num_workers: 10
  subsample: False # if True, use random 10 samples per subset

model:
  mode: classification # classification or regression
  max_length: 512
  vocab_size: 32100
  bidirectional: False
  hidden_size: 512
  embedding_size: 192
  padding_idx: 0
  num_layers: 1
  output_size: 6 # 1 if regression
  dropout: 0.1

training:
  epochs: 2
  validate: True
  val_check_interval: 0.5
  learning_rate: 0.001
  weight_decay: 0.1
  early_stopping: True
  patience: 0 # works only if early_stopping is True